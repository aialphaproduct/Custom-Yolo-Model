import streamlit as st
import numpy as np
import cv2
import tempfile
import os
import time
import gdown  # ThÆ° viá»‡n táº£i file tá»« Google Drive

# --- Táº£i yolov3.weights tá»« Google Drive náº¿u chÆ°a cÃ³ ---
def download_weights():
    file_id = "10ygsxRHye1DNgpErQZ6NghVIPhat6-UO"
    output_path = "model/yolov3.weights"

    if not os.path.exists(output_path):
        st.info("ðŸ“¥ Äang táº£i yolov3.weights tá»« Google Drive...")
        file_url = f"https://drive.google.com/uc?id={file_id}"
        gdown.download(file_url, output_path, quiet=False)
        st.success("âœ… ÄÃ£ táº£i xong yolov3.weights!")
    else:
        st.info("âœ”ï¸ File yolov3.weights Ä‘Ã£ cÃ³ sáºµn.")

download_weights()

# --- Load YOLO Model ---
@st.cache_resource
def load_yolo():
    try:
        model = cv2.dnn.readNetFromDarknet("model/yolov3.cfg", "model/yolov3.weights")
        layer_names = model.getLayerNames()
        output_layers = [layer_names[i - 1] for i in model.getUnconnectedOutLayers().flatten()]
        return model, output_layers
    except Exception as e:
        st.error(f"ðŸš¨ Lá»—i khi táº£i mÃ´ hÃ¬nh YOLO: {e}")
        return None, None

yolo_model, yolo_output_layers = load_yolo()

# --- Class Labels ---
class_labels = ["person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
                "trafficlight", "firehydrant", "stopsign", "parkingmeter", "bench", "bird", "cat",
                "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack",
                "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sportsball",
                "kite", "baseballbat", "baseballglove", "skateboard", "surfboard", "tennisracket",
                "bottle", "wineglass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
                "sandwich", "orange", "broccoli", "carrot", "hotdog", "pizza", "donut", "cake", "chair",
                "sofa", "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse",
                "remote", "keyboard", "cellphone", "microwave", "oven", "toaster", "sink", "refrigerator",
                "book", "clock", "vase", "scissors", "teddybear", "hairdrier", "toothbrush"]

# MÃ u cho bounding boxes
np.random.seed(42)
colors = np.random.randint(0, 255, size=(len(class_labels), 3), dtype="uint8")

# --- UI Streamlit ---
st.title("ðŸ–¼ï¸ YOLO Object Detection")
st.write("Táº£i áº£nh, video lÃªn hoáº·c sá»­ dá»¥ng webcam Ä‘á»ƒ nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng báº±ng YOLOv3.")

# --- Sidebar Ä‘á»ƒ chá»n cháº¿ Ä‘á»™ ---
option = st.sidebar.selectbox("Chá»n cháº¿ Ä‘á»™:", ["áº¢nh tÄ©nh", "Video", "Webcam (Real-time)"])

if option == "áº¢nh tÄ©nh":
    uploaded_file = st.file_uploader("ðŸ“¤ Chá»n áº£nh...", type=["jpg", "png", "jpeg"])

    if uploaded_file is not None and yolo_model:
        # ... (Pháº§n code xá»­ lÃ½ áº£nh tÄ©nh giá»¯ nguyÃªn) ...
        tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".jpg")
        tfile.write(uploaded_file.read())
        image_path = tfile.name

        with open(image_path, "rb") as f:
             img_array = np.asarray(bytearray(f.read()), dtype=np.uint8)
             img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)  # Giá»¯ nguyÃªn mÃ u gá»‘c

        img_height, img_width = img.shape[:2]
        img_blob = cv2.dnn.blobFromImage(img, 0.003922, (416, 416), swapRB=True, crop=False)
        yolo_model.setInput(img_blob)
        detection_layers = yolo_model.forward(yolo_output_layers)

        detected_objects = {}

        for layer in detection_layers:
            for detection in layer:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]

                if confidence > 0.5:
                    box = detection[:4] * np.array([img_width, img_height, img_width, img_height])
                    (centerX, centerY, width, height) = box.astype("int")
                    startX = int(centerX - (width / 2))
                    startY = int(centerY - (height / 2))
                    endX = startX + width
                    endY = startY + height

                    if class_id not in detected_objects:
                        detected_objects[class_id] = []

                    detected_objects[class_id].append({
                        "box": (startX, startY, endX, endY),
                        "confidence": confidence,
                        "center": (centerX, centerY)
                    })

        final_objects = []
        for class_id, objects in detected_objects.items():
            objects = sorted(objects, key=lambda x: x["confidence"], reverse=True)

            grouped_objects = []
            for obj in objects:
                centerX, centerY = obj["center"]

                found = False
                for group in grouped_objects:
                    existing_centerX, existing_centerY = group[0]["center"]
                    distance = np.sqrt((centerX - existing_centerX) ** 2 + (centerY - existing_centerY) ** 2)

                    if distance < 50:
                        found = True
                        break

                if not found:
                    grouped_objects.append([obj])

            for group in grouped_objects:
                best_object = max(group, key=lambda x: x["confidence"])
                final_objects.append(best_object)

        for obj in final_objects:
            (startX, startY, endX, endY) = obj["box"]
            confidence = obj["confidence"]
            class_id = next(key for key, val in detected_objects.items() if obj in val)
            color = [int(c) for c in colors[class_id]]
            label = f"{class_labels[class_id]}: {confidence:.2f}"

            cv2.rectangle(img, (startX, startY), (endX, endY), color, 2)
            cv2.putText(img, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        st.image(img, caption="ðŸ“ Káº¿t quáº£ nháº­n diá»‡n", use_column_width=True, channels="BGR")  # Hiá»ƒn thá»‹ áº£nh vá»›i mÃ u chÃ­nh xÃ¡c

        # --- LÆ°u áº£nh Ä‘Ã£ xá»­ lÃ½ vÃ o tá»‡p táº¡m thá»i ---
        temp_save_path = tempfile.NamedTemporaryFile(delete=False, suffix=".jpg").name
        cv2.imwrite(temp_save_path, img)  # Giá»¯ nguyÃªn mÃ u khi lÆ°u

        # --- Táº¡o nÃºt táº£i xuá»‘ng áº£nh ---
        with open(temp_save_path, "rb") as file:
            st.download_button(
                label="ðŸ“¥ Táº£i áº£nh káº¿t quáº£",
                data=file,
                file_name="yolo_detection_result.jpg",
                mime="image/jpeg"
            )

        # XÃ³a file áº£nh táº¡m thá»i
        for _ in range(3):
            try:
                if os.path.exists(image_path):
                    os.remove(image_path)
                if os.path.exists(temp_save_path):
                    os.remove(temp_save_path)
                break
            except PermissionError:
                time.sleep(2)

elif option == "Video":
    uploaded_file = st.file_uploader("ðŸ“¤ Chá»n video...", type=["mp4", "avi", "mov"])

    if uploaded_file is not None and yolo_model:
        tfile = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
        tfile.write(uploaded_file.read())

        video_capture = cv2.VideoCapture(tfile.name)

        # --- Placeholder cho video output ---
        video_placeholder = st.empty()

        # --- NÃºt download (khá»Ÿi táº¡o trÆ°á»›c, Ä‘á»ƒ cáº­p nháº­t sau) ---
        download_button_placeholder = st.empty()

         # --- Ghi video Ä‘áº§u ra ---
        fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Hoáº·c *'XVID'
        temp_output_path = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False).name
        out = cv2.VideoWriter(temp_output_path, fourcc, 20.0, (int(video_capture.get(3)), int(video_capture.get(4))))


        while video_capture.isOpened():
            ret, frame = video_capture.read()
            if not ret:
                break  # Káº¿t thÃºc khi háº¿t video

            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img_height, img_width = frame.shape[:2]
            img_blob = cv2.dnn.blobFromImage(frame, 0.003922, (416, 416), swapRB=True, crop=False)

            yolo_model.setInput(img_blob)
            detection_layers = yolo_model.forward(yolo_output_layers)

             # --- Pháº§n xá»­ lÃ½ detection vÃ  váº½ bounding box (tÆ°Æ¡ng tá»± nhÆ° pháº§n áº£nh tÄ©nh) ---
            detected_objects = {}

            for layer in detection_layers:
               for detection in layer:
                  scores = detection[5:]
                  class_id = np.argmax(scores)
                  confidence = scores[class_id]
                  if confidence > 0.5:
                     box = detection[0:4] * np.array([img_width, img_height, img_width, img_height])
                     (centerX, centerY, width, height) = box.astype("int")

                     startX = int(centerX - (width / 2))
                     startY = int(centerY - (height / 2))
                     endX = int(startX + width)
                     endY = int(startY + height)

                     if class_id not in detected_objects:
                        detected_objects[class_id]=[]
                     detected_objects[class_id].append({"box": (startX,startY, endX, endY), "confidence": float(confidence), "center" : (centerX, centerY)})
            final_objects = []
            for class_id, objects in detected_objects.items():
               objects = sorted(objects, key=lambda x: x["confidence"], reverse = True)
               grouped_objects = []
               for obj in objects:
                  centerX, centerY = obj["center"]
                  found=False
                  for group in grouped_objects:
                     existing_centerX, existing_centerY = group[0]["center"]
                     distance = np.sqrt((centerX - existing_centerX)**2 + (centerY- existing_centerY)**2)
                     if distance < 50: #ngÆ°á»¡ng khoáº£ng cÃ¡ch
                        found = True
                        break
                  if not found:
                     grouped_objects.append([obj])
               for group in grouped_objects:
                  best_object = max(group, key=lambda x:x["confidence"])
                  final_objects.append(best_object)

            for obj in final_objects:
               startX, startY, endX, endY = obj["box"]
               confidence = obj["confidence"]
               class_id = next(key for key, value in detected_objects.items() if obj in value)
               color = [int(c) for c in colors[class_id]]
               label = "%s: %.2f" % (class_labels[class_id], confidence)
               cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)
               cv2.putText(frame, label, (startX, startY-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

            # --- Hiá»ƒn thá»‹ frame lÃªn placeholder ---
            video_placeholder.image(frame, channels="RGB")

            # --- Ghi frame vÃ o video Ä‘áº§u ra ---
            out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)) # Ghi frame Ä‘Ã£ xá»­ lÃ½

        video_capture.release()
        out.release()

        # --- Cáº­p nháº­t nÃºt download ---
        with open(temp_output_path, "rb") as file:
            download_button_placeholder.download_button(
                label="ðŸ“¥ Táº£i video káº¿t quáº£",
                data=file,
                file_name="yolo_detection_video.mp4",
                mime="video/mp4"
            )
        # --- Dá»n dáº¹p tá»‡p táº¡m ---
        try:
            os.remove(tfile.name)
            os.remove(temp_output_path)
        except Exception as e:
            st.error(f"Lá»—i khi xÃ³a tá»‡p táº¡m thá»i: {e}")

elif option == "Webcam (Real-time)":
    st.write("### ðŸ“¹ Nháº­n diá»‡n Ä‘á»‘i tÆ°á»£ng qua Webcam")
    run = st.checkbox("â–¶ï¸ Báº¯t Ä‘áº§u / Dá»«ng")
    FRAME_WINDOW = st.image([])  # Placeholder Ä‘á»ƒ hiá»ƒn thá»‹ khung hÃ¬nh
    camera = cv2.VideoCapture(0)  # Sá»­ dá»¥ng webcam máº·c Ä‘á»‹nh (index 0)

    while run and yolo_model:
        ret, frame = camera.read()
        if not ret:
            st.error("KhÃ´ng thá»ƒ truy cáº­p webcam.")
            break

        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Chuyá»ƒn sang RGB Ä‘á»ƒ hiá»ƒn thá»‹ Ä‘Ãºng mÃ u
        img_height, img_width = frame.shape[:2]
        img_blob = cv2.dnn.blobFromImage(frame, 0.003922, (416, 416), swapRB=True, crop=False)

        yolo_model.setInput(img_blob)
        detection_layers = yolo_model.forward(yolo_output_layers)

        # --- Xá»­ lÃ½ detection vÃ  váº½ bounding box ---
        detected_objects = {}

        for layer in detection_layers:
           for detection in layer:
              scores = detection[5:]
              class_id = np.argmax(scores)
              confidence = scores[class_id]
              if confidence > 0.5:
                 box = detection[0:4] * np.array([img_width, img_height, img_width, img_height])
                 (centerX, centerY, width, height) = box.astype("int")

                 startX = int(centerX - (width / 2))
                 startY = int(centerY - (height / 2))
                 endX = int(startX + width)
                 endY = int(startY + height)

                 if class_id not in detected_objects:
                    detected_objects[class_id]=[]
                 detected_objects[class_id].append({"box": (startX,startY, endX, endY), "confidence": float(confidence), "center" : (centerX, centerY)})
        final_objects = []
        for class_id, objects in detected_objects.items():
           objects = sorted(objects, key=lambda x: x["confidence"], reverse = True)
           grouped_objects = []
           for obj in objects:
              centerX, centerY = obj["center"]
              found=False
              for group in grouped_objects:
                 existing_centerX, existing_centerY = group[0]["center"]
                 distance = np.sqrt((centerX - existing_centerX)**2 + (centerY- existing_centerY)**2)
                 if distance < 50: #ngÆ°á»¡ng khoáº£ng cÃ¡ch
                    found = True
                    break
              if not found:
                 grouped_objects.append([obj])
           for group in grouped_objects:
              best_object = max(group, key=lambda x:x["confidence"])
              final_objects.append(best_object)

        for obj in final_objects:
           startX, startY, endX, endY = obj["box"]
           confidence = obj["confidence"]
           class_id = next(key for key, value in detected_objects.items() if obj in value)
           color = [int(c) for c in colors[class_id]]
           label = "%s: %.2f" % (class_labels[class_id], confidence)
           cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)
           cv2.putText(frame, label, (startX, startY-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        FRAME_WINDOW.image(frame)  # Cáº­p nháº­t khung hÃ¬nh lÃªn placeholder

    camera.release()
    st.write("Káº¿t thÃºc.")